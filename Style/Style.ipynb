{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anitagold/SPAIC_Hungary/blob/master/Style/Style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAfz6YmQDWXu"
   },
   "source": [
    "# Style Transfer with Deep Neural Networks\n",
    "\n",
    "\n",
    "## Concept\n",
    "The Style Transfer means changing the style of one picture from another. Every picture can split into two big parts:\n",
    "* content\n",
    "* style\n",
    "\n",
    "If you imagine a famous paintings, for example Sunflowers By Van Gogh, you are able two determine hese parts: content is the sunflowers, the vase and the desk or in one word: the composition. Style means the colour, the lines of paintings, textures, the unique marks of painter. This works not just paintings, but every pictures. With this Jupiter Notebook Code you will be able to create style transfered picture. For this you have to choose two pictures: one for style and one for the source of content. Don't worry, if you have no ideas, we provide some of them. Feel free to try it and change the source data and the modell as well.\n",
    "\n",
    "## Idea\n",
    "This style transfer method based in Udacity course and this paper: [Image Style Transfer Using Convolutional Neural Networks, by Gatys](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).\n",
    "\n",
    "In this paper, style transfer uses the 19-layer VGG Network. Our program is a generalized version of this. The user can upload photos for style and content image, and can choose which model to use. The user can set several parameters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NQMmgryTDWXv"
   },
   "outputs": [],
   "source": [
    "# import resources\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mktz9_E9DWXx"
   },
   "source": [
    "## Load in VGG19 (features)\n",
    "\n",
    "VGG19 is splitted into two parts:\n",
    "* `features`: convolutional and pooling layers\n",
    "* `classifier`: the three linear and classifier layers at the end\n",
    "\n",
    "We only need the `features` part, that we load in and \"freeze\" the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuSj8Hf9DWXy"
   },
   "outputs": [],
   "source": [
    "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "# freeze all VGG parameters since we're only optimizing the target image\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "1eRuKserDWXz",
    "outputId": "86bee218-a3d9-4206-ae4e-80e5e3994524"
   },
   "outputs": [],
   "source": [
    "# move the model to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0RUzvXWDWX2"
   },
   "source": [
    "### Load Content and Style Images\n",
    "\n",
    "In this part you are able to load the source pictures. It's ideal when the x and y dimensions of your picture are the same. However, you don't have to worry about resulution, the next function will be transform them to the right size.\n",
    "\n",
    "This funcition transforms your pictures and converts them to normalized tensors. However, the name of funcion is load_image, the source directories and files will be set up in another part of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Znt3UT2DWX3"
   },
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    ''' Load in and transform an image, making sure the image\n",
    "       is <= 400 pixels in the x-y dims.'''\n",
    "    if \"http\" in img_path:\n",
    "        response = requests.get(img_path)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # large images will slow down processing\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    in_transform = transforms.Compose([\n",
    "                        transforms.Resize(size),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
    "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yu3NieB0DWX7"
   },
   "outputs": [],
   "source": [
    "# helper function to show un-normalizied images:\n",
    "# It converts pictures from Tensor images to a NumPy images for display\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Display a tensor as an image. \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2S7Zs_bZDWX5"
   },
   "source": [
    "Loading images by file name and forcing the style image to be the same size as the content image.\n",
    "\n",
    "### Instructions\n",
    "Don't forget to set up the source data properly. Be careful, the default setting of main dir works with Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ltP8LIcDWX5"
   },
   "outputs": [],
   "source": [
    "# defining main directory, style, content and target picture\n",
    "maindir = '/content/drive/My Drive/Colab Notebooks/Style'\n",
    "content_file = maindir + './images/[PICTURE_CONTENT].jpg'\n",
    "style_file = maindir + './images/[PICTURE_STYLE].jpg'\n",
    "target_file = maindir + './target/[PICTURE_TARGET].jpg' \n",
    "\n",
    "# load in content and style image\n",
    "content = load_image(content_file).to(device)\n",
    "\n",
    "# resize style to match content\n",
    "style = load_image(style_file, shape=content.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "id": "0HZcv785DWX9",
    "outputId": "cefeab6c-1e40-4e31-f6d8-ac62a36ecfd6"
   },
   "outputs": [],
   "source": [
    "# display the images\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# content and style ims side-by-side\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4rDoXf0DWX_"
   },
   "source": [
    "---\n",
    "## VGG19 Layers\n",
    "\n",
    "We have to pass forward the source image through the VGG19 network until we get to the desired layer(s) and then get the output from that layer.\n",
    "\n",
    "If you unremarked the next 'print(vgg)' command, you will see the names of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NqTeFpDDWX_"
   },
   "outputs": [],
   "source": [
    "# print out VGG19 structure so you can see the names of various layers\n",
    "# print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mATDqx8GDWYC"
   },
   "source": [
    "## Content and Style Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7mdR_Y1DWYD"
   },
   "outputs": [],
   "source": [
    "def get_features(image, model, layers=None):\n",
    "    \n",
    "    # Layers\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1', \n",
    "                  '10': 'conv3_1', \n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',  ##  This is for content representation\n",
    "                  '28': 'conv5_1'}\n",
    "        \n",
    "    features = {}\n",
    "    x = image    \n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VX-5JWjLDWYF"
   },
   "source": [
    "---\n",
    "## Gram Matrix \n",
    "\n",
    "The output of convolutional layers is a tensor. The dimension of this tensor are connected to the `batch_size`, a depth, `d` and some height and width (`h`, `w`). If you want to know ore about it: https://en.wikipedia.org/wiki/Gramian_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHfMI638DWYF"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \n",
    "    _, d, h, w = tensor.size()\n",
    "    \n",
    "    tensor = tensor.view(d, h * w)\n",
    "    \n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "SK-rE7PaDWYI"
   },
   "source": [
    "## Make it all parts together\n",
    "\n",
    "In this section we collect the contect and style features, make a style representation and create a target image. The target image is the clone of content of original picture with the style from style source. This style will be changed over each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1c1jf2vCDWYJ"
   },
   "outputs": [],
   "source": [
    "# get content and style features before training\n",
    "content_features = get_features(content, vgg)\n",
    "style_features = get_features(style, vgg)\n",
    "\n",
    "# style representation\n",
    "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "# create target image and pass it to device\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NbGLw_eCDWYL"
   },
   "source": [
    "---\n",
    "## Losses and Weights\n",
    "\n",
    "#### Style weights of each layer\n",
    "\n",
    "In this section you can modify the style representation of each convulutional layer. That weight range is advised to be between 0 and 1. Changing these values affect the outlook of target image and the computing time. If earlier layers get higher values, the target image get larger or more style artifacts.\n",
    "\n",
    "#### Portionised between content and style weight\n",
    "\n",
    "Alpha (`content_weight`) and a beta (`style_weight`) represent the ratio, that will affect how stylized the final image is. Default value  of alpha equals 1, but you are free to change it and see the result. Set the beta to get your desired ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08CpaqiIDWYL"
   },
   "outputs": [],
   "source": [
    "# weights for each style layer \n",
    "style_weights = {'conv1_1': 1.0,\n",
    "                 'conv2_1': 0.75,\n",
    "                 'conv3_1': 0.1,\n",
    "                 'conv4_1': 0.05,\n",
    "                 'conv5_1': 0.05}\n",
    "\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 0.1  # beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QekbxN-kDWYN"
   },
   "source": [
    "## One step before ending\n",
    "\n",
    "In this section we update the target. If the result is far away your imagination, you can stop the process and change the settings. \n",
    "\n",
    "We recommend to test the code over 1 or 2 epochs. For the good result you should itarate your target picutre over at least 2000 epochs. Don't forget, changing values causes different result such as playing with number of epochs. \n",
    "\n",
    "\n",
    "###  Inside the loop: Loss \n",
    "\n",
    "We use 3 differents losses: content, style and target or total loss. The loss value represents the gap between present state and the desirered state. The calculating method is similiar with other ML examples. The only exception is the total loss. The total loss is the sum fo content and style loss biased with ratio of alpha and beta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oSiyROA3DWYN",
    "outputId": "b5100b77-0650-4f96-eab2-077664ee0ada",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for displaying the target image, intermittently\n",
    "show_every = 400\n",
    "\n",
    "# iteration hyperparameters\n",
    "optimizer = optim.Adam([target], lr=0.003)\n",
    "steps = 2000\n",
    "\n",
    "for ii in range(1, steps+1):\n",
    "    \n",
    "    # get the features from target image\n",
    "    target_features = get_features(target, vgg)\n",
    "    \n",
    "    # the content loss\n",
    "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "    \n",
    "    # the style loss\n",
    "    style_loss = 0\n",
    "    for layer in style_weights:\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        style_gram = style_grams[layer]\n",
    "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "    # calculate the total loss\n",
    "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "    \n",
    "    # update target image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # display images during proecessing and print the connected loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(im_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWiK2eTXDWYP"
   },
   "source": [
    "## Display the Target Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "id": "O43o78mZDWYQ",
    "outputId": "04f7be4b-4e3a-44c1-f5aa-7e3aebd1c06e"
   },
   "outputs": [],
   "source": [
    "# display content and final stage of target image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtgcBmgk-TDk"
   },
   "source": [
    "##Save the target picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pxaxgjwe-PUY"
   },
   "outputs": [],
   "source": [
    "\n",
    "save_image(target, target_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Style.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
